---
title: "Politeness and Coordination"
author: "A. Paxton, L. Brown, & B. Winter"
output:
  html_document:
    keep_md: yes
    number_sections: yes
---

This R markdown provides the data preparation for our project analyzing how 
power and politeness affect interpersonal movement synchrony during a variety 
of interaction tasks (Paxton, Brown, & Winter, *in preparation*). 

To run this from scratch, you will need the following files:

* `./data/movement_dataframes-aggregated/`: Directory of movement data 
  derived from videos (30 FPS) using a frame-differencing method (e.g., Paxton 
  & Dale, 2013, *Behavior Research Methods*). De-identified movement data are 
  freely available in the OSF repository for the project: `https://osf.io/wrzf2/`.
* `./scripts/03-data_analysis/required_packages-pac.r`: Installs required libraries, 
  if they are not already installed. **NOTE**: This should be run *before* running 
  this script.
* `./scripts/03-data_analysis/libraries_and_functions-pac.r`: Loads in necessary 
  libraries and creates new functions for our analyses.

Additional files will be created during the initial run that will help reduce processing 
time. Many of these are available as CSVs from the OSF repository listed above.

**Code written by**: A. Paxton (University of Connecticut)

**Date last modified**: 20 April 2019

***

# Data import

**NOTE**: The chunks of code in this section do not have to be run each time, 
since the resulting datasets will be saved to CSV files. As a result, these 
chunks are currently set to `eval=FALSE`. Bear this in mind if these data 
need to be re-calculated.

***

## Preliminaries

This section clears the workspace, loads (and, if needed, installs) required 
packages, and specifies important global variables.

```{r prep-prelim, warning = FALSE, error = FALSE, message = FALSE}

# clear our workspace
rm(list=ls())

# # if needed, uncomment this to install libraries
# source('./scripts/03-data_analysis/required_packages-pac.r')

# read in libraries and create functions
source('./scripts/03-data_analysis/libraries_and_functions-pac.r')

```

***

## Pre-process and concatenate movement data

After initial video processing, each video clip produced its own file of movement
data. Here, we first pre-process the data by downsampling to 10Hz (consistent with previous
research on movement coordination; cf. Paxton & Dale, 2013, *Behavior Research Methods*).
We then remove first few seconds and last few seconds of the data from each conversation
and concatenate all conversations' movement data into a single file.

```{r preprocess-concatenate-data, warning = FALSE, error = FALSE, message = FALSE, eval = FALSE}

# get list of individual conversations included in the data
conversation_files = list.files('./data/movement_dataframes-aggregated',
                                full.names = TRUE)

# specify Butterworth filters
anti_aliasing_butter = signal::butter(4,.4)
post_downsample_butter = signal::butter(2,.02)

# process each file
conversation_df = data.frame()
for (conversation_file in conversation_files){
  
  # read in next file
  next_conversation = read.table(conversation_file, sep=',',
                                 header=TRUE, stringsAsFactors = FALSE)
  
  # split info from dyad
  if (!grepl("Map", conversation_file)){
    next_conversation = next_conversation %>% ungroup() %>%
      separate(dyad, into = c('participant_id',
                              'participant_gender',
                              'partner_type',
                              'task',
                              'condition'), sep='_') %>%
      
      # remove .mov extension from condition
      mutate(condition = sub(pattern = "(.*?)\\..*$", replacement = "\\1", condition)) 
    
  } else {
    next_conversation = next_conversation %>% ungroup() %>%
      separate(dyad, into = c('participant_id',
                              'participant_gender',
                              'partner_type',
                              'task',
                              'condition',
                              'map_cond'), sep='_') %>%
      
      # fix the task information to include who led (participant vs. interlocutor)
      unite(task, c('task','map_cond')) %>%
      mutate(task = sub(pattern = "(.*?)\\..*$", replacement = "\\1", task)) 
  }
  
  # movement processing
  next_conversation = next_conversation %>% ungroup() %>%
    
    # delineate movement between target participant (1 / L) and partner (0 / R)
    mutate(participant = ifelse(participant=='left',
                                1,
                                0)) %>%
    
    # create a time variable from the difference number
    mutate(t = difference_number / raw_sampling_rate) %>%
    
    # filter and downsample
    dplyr::group_by(participant, partner_type, task, condition) %>%
    mutate(movement = signal::filtfilt(anti_aliasing_butter, movement)) %>%
    mutate(t = floor(t * downsampled_sampling_rate) / downsampled_sampling_rate) %>%
    ungroup() %>%
    dplyr::group_by(participant, partner_type, task, condition, t) %>%
    mutate(movement = mean(movement)) %>%
    slice(1) %>%
    ungroup() %>%
    dplyr::group_by(participant) %>%
    mutate(movement = signal::filtfilt(post_downsample_butter, movement)) %>%
    ungroup() %>%
    
    # drop unneeded variable and rename participant
    select(-difference_number) %>%
    dplyr::rename(interlocutor = participant) %>% 
    
    # trim the beginning and end from each conversation (instructions and wrap-up time)
    dplyr::group_by(participant_id, interlocutor) %>%
    filter(t > trimmed_time) %>%
    filter(t < (max(t) - trimmed_time)) %>%
    arrange(t) %>%
    ungroup()
  
  # save plot of movement with beginning and end trimmed
  trimmed_movement_plot = ggplot(next_conversation,
                                 aes(x = t,
                                     y = movement,
                                     color = as.factor(interlocutor))) +
    geom_line() +
    geom_path() +
    theme(legend.position="bottom")
  ggsave(plot = trimmed_movement_plot,
         height = 3,
         width = 3,
         filename = paste0('./figures/movement-trimmed/pac-trimmed_movement-',
                           basename(conversation_file),'.jpg'))
  
  # append dataframe to group files
  conversation_df = rbind.data.frame(conversation_df, 
                                     next_conversation)
}

# save aggregated file
write.table(conversation_df,
            './data/pac-filtered_movement_data.csv',
            sep = ",", append = FALSE, row.names = FALSE, col.names = TRUE)

```

## Summarize conversation information

Now that we've prepared the data for recurrence analyses, let's find 
out a bit more about the conversations.

```{r invisible-load-trimmed-for-stats, echo = FALSE}

# invisible chunk: load in the conversation data to correctly display table if everything else is `eval=FALSE`
conversation_df = read.table('./data/pac-filtered_movement_data.csv',
                            sep=',',header=TRUE)

```

```{r summarize-info}

summary_stats = conversation_df %>% ungroup() %>%
  dplyr::group_by(participant_id, task) %>%
  summarise(duration = max(t)) %>%
  mutate(num_samples = duration * downsampled_sampling_rate) %>%
  mutate(max_time = round(duration / 60, 2)) %>%
  ungroup()

```

```{r summary-range-duration}

# what's the range of conversation data (in minutes) by conversation type?
summary_duration = summary_stats %>% ungroup() %>%
  dplyr::group_by(task) %>%
  summarise(min_duration = min(max_time),
            max_duration = max(max_time),
            mean_duration = mean(max_time))
print(summary_duration)

```

```{r plot-conversation-summary-stats, echo=FALSE, warning = FALSE, error = FALSE, message = FALSE, eval = FALSE}

# separate by different tasks
map_p_summary_df = filter(summary_stats, task=="Map_I")
map_i_summary_df = filter(summary_stats, task=="Map_I")
convo_summary_df = filter(summary_stats, task=="Convo")
tweety_summary_df = filter(summary_stats, task=="Tweety")
role_summary_df = filter(summary_stats, task=="Role")
  
# create histograms for each
map_p_summary_hist = qplot(map_p_summary_df$max_time,
                         geom = 'histogram', bins = 30) +
  xlim(c(0,15)) + ylab('Count') + xlab('Duration (min)') + ggtitle('Map task: Participant leads')
map_i_summary_hist = qplot(map_i_summary_df$max_time,
                         geom = 'histogram', bins = 30) +
  xlim(c(0,15)) + ylab('Count') + xlab('Duration (min)') + ggtitle('Map task: Partner leads')
convo_summary_hist = qplot(convo_summary_df$max_time,
                           geom = 'histogram', bins = 30) +
  xlim(c(0,15)) + ylab('Count') + xlab('Duration (min)') + ggtitle('Conversation')
tweety_summary_hist = qplot(tweety_summary_df$max_time,
                            geom = 'histogram', bins = 30) +
  xlim(c(0,15)) + ylab('Count') + xlab('Duration (min)') + ggtitle('Story retelling')
role_summary_hist = qplot(role_summary_df$max_time,
                          geom = 'histogram', bins = 30) +
  xlim(c(0,15)) + ylab('Count') + xlab('Duration (min)') + ggtitle('Role-playing')

# save smaller version for knitr
ggsave('./figures/summary_info/pac-conversation_lengths.png',
       units = "in", width = 4, height = 7, dpi=100,
       grid.arrange(
         top = textGrob("Duration histograms by conversation type",
                        gp=gpar(fontsize=14)),
         map_p_summary_hist,
         map_i_summary_hist,
         convo_summary_hist,
         tweety_summary_hist,
         role_summary_hist,
         ncol = 1
       ))


```

![**Figure**. Histograms for conversation duration by conversation type.](./figures/summary_info/pac-conversation_lengths.png)

***

# Recurrence analyses

**NOTE**: The chunks of code in this section do not have to be run each time, 
since the resulting datasets will be saved to CSV files. As a result, these 
chunks are currently set to `eval=FALSE`. Bear this in mind if these data need 
to be re-calculated.

***

## Preliminaries

This section clears the workspace and reads in the prepared data files.

```{r recurrence-prelims, warning = FALSE, error = FALSE, message = FALSE, eval = FALSE}

# clear our workspace
rm(list=ls())

# read in libraries and functions again
source('./scripts/03-data_analysis/libraries_and_functions-pac.r')

# read in data
conversation_df = read.table('./data/pac-filtered_movement_data.csv',
                            sep=',',header=TRUE) 

```

***

## Identify CRQA parameters

Before we can analyze the data, we need to identify the appropriate parameters 
for continuous CRQA for the dataset. We identify parameters that provide a 
steady *rate of recurrence* or *RR* of approximately 5% for each conversation 
and save these parameters to a CSV file.

```{r crqa-parameters, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, eval = FALSE}

# run CRQA parameters
source('./scripts/03-data_analysis/continuous_rqa_parameters-pac.r')

```

***

## Prepare for CRQA and DRPs

```{r ready-for-crqa-drps, eval = FALSE}

# read in our crqa parameters
conversation_df_crqa = read.table('./data/crqa_data_and_parameters-pac.csv',
                         sep=',',header=TRUE) %>%
  select(-rr, -from.target, -movement_0, -movement_1)
  
# grab other information from the original df
supplemental_info = conversation_df %>% ungroup() %>%
  select(participant_id, partner_type, task, participant_gender, condition) %>%
  distinct() %>%
  
  # convert to partner type to numeric
  mutate(partner_type_str = partner_type) %>%
  mutate(partner_type = ifelse(partner_type=='Friend',
                               0,      # friend = 0
                               1)) %>% # prof = 1
  
  # convert to gender to numeric
  mutate(participant_gender_str = participant_gender) %>%
  mutate(participant_gender = ifelse(participant_gender=='F',
                                     0,      # female = 0
                                     1)) %>% # male = 1
  
  # convert to condition to numeric
  mutate(condition_str = condition) %>%
  mutate(condition = ifelse(condition=='AB',
                            0,      # AB = 0
                            1)) %>% # BA = 1
  
  # convert to task to numeric
  mutate(task_str = task) %>%
  mutate(task = ifelse(task=='Convo',
                       1,                                  # Convo = 1
                       ifelse(task=='Map_P',
                              2,                           # Map, participant-lead = 2
                              ifelse(task=='Map_I', 
                                     3,                    # Map, partner-led = 3
                                     ifelse(task=='Role',
                                            4,             # Role = 4
                                            5)))))          # Tweety = 5
         
# combine both
conversation_df_crqa = full_join(supplemental_info,
                                  conversation_df_crqa,
                                 by = c("participant_id",
                                        "task_str"="task",
                                        "partner_type_str"="partner_type"))

```

***

## Run CRQA and DRPs

Now that we have our parameters, we run continuous CRQA over each conversation
for each dyad using the `crqa` function from the `crqa` package (Coco & Dale, 
2014, *Frontiers in Psychology*).

The time series are entered into the `crqa` function to allow us to interpret
the eventual diagonal recurrence profiles in the same way that the videos were
taken: Participant-leading dynamics will be reflected in the left half of the
plot, and partner-leading dynamics will be reflected in the right half of the
plot.

```{r run-crqa-using-parameters, eval = FALSE}

# identify window size
target_seconds = 5
win_size = target_seconds * downsampled_sampling_rate

# slice up the data so that we have one dataset per conversation
split_convs = split(conversation_df_crqa,
                    list(conversation_df_crqa$participant_id, 
                         conversation_df_crqa$partner_type,
                         conversation_df_crqa$task))

# cycle through each conversation using the sliced subsets
drp_results = data.frame()
crqa_results = data.frame()
for (next_conv in split_convs){
  if (dim(next_conv)[1] > 0){
    
    # isolate parameters for this next dyad
    chosen.delay = unique(next_conv$chosen.delay)
    chosen.embed = unique(next_conv$chosen.embed)
    chosen.radius = unique(next_conv$chosen.radius)
    
    # get basic info
    participant_id = unique(next_conv$participant_id)
    partner_type = unique(next_conv$partner_type)
    partner_type_str = unique(next_conv$partner_type_str)
    task = unique(next_conv$task)
    task_str = unique(next_conv$task_str)
    
    # print update
    print(paste0("CRQA: Participant ", participant_id,
                ", partner ", partner_type_str,
                ", task ", task_str))
    
    # run cross-recurrence
    # order time series so that they reflect actual seating in experiment
    # (left-leading = participant-leading; right-leading = partner-leading)
    rec_analysis = crqa(ts1=next_conv$rescale.movement_1, # participant
                        ts2=next_conv$rescale.movement_0, # partner
                        delay=chosen.delay,
                        embed=chosen.embed,
                        r=chosen.radius,
                        normalize=0, 
                        rescale=0, 
                        mindiagline=2,
                        minvertline=2, 
                        tw=0, 
                        whiteline=FALSE,
                        recpt=FALSE)
    
    # save plot-level information to dataframe
    next_data_line = data.frame(c(participant_id,
                                  partner_type,
                                  task,
                                  rec_analysis[1:9]))
    names(next_data_line) = c("participant_id", 'partner_type', 'task',
                              names(rec_analysis[1:9]))
    crqa_results = rbind.data.frame(crqa_results,next_data_line)
  
    # recreate DRP from diagonal lines within our target window
    diag_lines = spdiags(rec_analysis$RP)
    subset_plot = data.frame(diag_lines$B[,diag_lines$d >= -win_size & diag_lines$d <= win_size])
    rr = colSums(subset_plot)/dim(subset_plot)[1]
  
    # convert to dataframe, padding (with 0) where no RR was observed
    next_drp = dplyr::full_join(data.frame(lag = as.integer(stringr::str_replace(names(rr),'X',''))-(win_size+1),
                                           rr = rr),
                         data.frame(lag = -win_size:win_size),
                         by='lag')
    next_drp[is.na(next_drp)] = 0
  
    # save it to dataframe
    next_drp$participant_id = participant_id
    next_drp$partner_type = partner_type
    next_drp$task = task
    drp_results = rbind.data.frame(drp_results,next_drp)
}}

# save results to file
write.table(crqa_results,'./data/crqa_results-pac.csv',sep=",")
write.table(drp_results,'./data/drp_results-pac.csv',sep=',')

```

***

## Export merged recurrence dataset

```{r merge-recurrence-datasets, eval = FALSE}

# merge CRQA and DRP analysis results
recurrence_results = full_join(drp_results, crqa_results,
                               by=c("participant_id", 'partner_type', 'task'))

# merge recurrence analyses and condition information
recurrence_df = full_join(recurrence_results, supplemental_info,
                          by=c("participant_id", 'partner_type', 'task'))

# save to file
write.table(recurrence_df,'./data/recurrence_df-pac.csv',sep=',')

```

***

# Data preparation

Now that we've calculated our CRQA and DRP measures, we're ready to prepare our data for analysis.

**NOTE**: The chunks of code in this section do not have to be run each time, 
since the resulting datasets will be saved to CSV files. As a result, these chunks 
are currently set to `eval=FALSE`. Bear this in mind if these data need to be re-calculated.

***

## Preliminaries

This section clears the workspace and reads in the prepared data files.

```{r prep-data-prep, warning=FALSE, error=FALSE, message=FALSE, eval = FALSE}

# clear our workspace
rm(list=ls())

# read in libraries and create functions
source('./scripts/03-data_analysis/libraries_and_functions-pac.r')

# read in the recurrence dataframe
recurrence_df = read.table('./data/recurrence_df-pac.csv', sep=',', header=TRUE)

```

***

## Create first- and second-order polynomials

In order to examine the linear and curvilinear patterns in the DRPs (cf. 
Main, Paxton, & Dale, 2016, *Emotion*), we create orthogonal polynomials 
for the lag term. This section creates the first- and second-order 
othogonal polynomials that are essential to allowing us to interpret the 
linear (i.e., first-order polynomial) and quadratic (i.e., second-order 
polynomial) patterns in the DRP independently from one another.

```{r create-polynomials, eval = FALSE}

# create first- and second-order orthogonal polynomials for lag
raw_lag = min(recurrence_df$lag):max(recurrence_df$lag)
lag_vals = data.frame(raw_lag)
lag_offset = (0-min(raw_lag)) + 1
t = stats::poly((raw_lag + lag_offset), 2)
lag_vals[, paste("ot", 1:2, sep="")] = t[lag_vals$raw_lag + lag_offset, 1:2]

# join it to the original data table
recurrence_df = left_join(recurrence_df,lag_vals, by = c("lag" = "raw_lag"))

```

***
 
## Specify precision

```{r specify-precision, eval=FALSE}

recurrence_df = recurrence_df %>% ungroup() %>%
  mutate_all(list(sprintf("%.14f",.)))

```

***

## Export dataframe

```{r export-dataframe, eval = FALSE}

# export dataframe
dput(recurrence_df, './data/plotting_df-pac.csv')

```

***

# Data analysis

Our data are clean, our parameters are identified, and our dataframe
is ready to go. We now analyze our data and generate visualizations.

***

## Preliminaries

This section clears the workspace and reads in the prepared data files.

```{r analysis-prelim, warning=FALSE, error=FALSE, message=FALSE}

# clear our workspace
rm(list=ls())

# read in libraries and create functions
source('./scripts/03-data_analysis/libraries_and_functions-pac.r')

# read in the plotting dataframe
rec_plot = dget('./data/plotting_df-pac.csv') %>% mutate_all(funs(as.numeric(.)))

# make sure we treat factors as factors
rec_plot = rec_plot %>% ungroup() %>%
  mutate_at(vars(one_of(factor_variables)),
            funs(as.factor))

```

***

## Recurrence by lag, partner type, and task

We now create a linear mixed-effects model to gauge how 
linear lag (`ot1`) and quadratic lag (`ot2`) interact with 
partner type (`partner_type`) and task (`task`) to 
influence overall body movement coordination (`rr`).

```{r rr-parner-task-gca-raw}

# model
rr_partner_task_gca_raw = lmer(rr ~ ot1 * ot2 * partner_type * task +
                                 (1 + partner_type | participant_id),
                               data = rec_plot, REML = FALSE)

# readable output
pander_lme(rr_partner_task_gca_raw)

```

*** 

## Exploring interaction terms

In order to understand the higher-order interactions, we 
create separate datasets for the friend and professor 
conversations and then re-run the model as above.

### Create separate dataframes

```{r create-separate-dataframes}

# create friend raw dataframe
friend_raw = rec_plot %>% ungroup() %>%
  mutate_all(funs(as.numeric)) %>%
  dplyr::filter(partner_type == min(partner_type))

# create professor raw dataframe
professor_raw = rec_plot %>% ungroup() %>%
  mutate_all(funs(as.numeric)) %>%
  dplyr::filter(partner_type == max(partner_type))

# convert to factors
friend_raw = friend_raw %>% ungroup() %>% 
  mutate_at(vars(one_of(factor_variables)), funs(as.factor))
professor_raw = professor_raw %>% ungroup() %>% 
  mutate_at(vars(one_of(factor_variables)), funs(as.factor))

```

### Friend-only model

```{r friend-model-raw}

# friends: unstandardized (raw) model
rr_friend_task_gca_raw = lmer(rr ~ ot1 * ot2 * task + 
                                (1 | participant_id),
                              data = friend_raw, REML = FALSE)
pander_lme(rr_friend_task_gca_raw)

```

### Professor-only model

```{r prof-model-raw}

# professor: unstandardized (raw) model
rr_prof_task_gca_raw = lmer(rr ~ ot1 * ot2 * task +
                                (1 | participant_id),
                              data = professor_raw, REML = FALSE)
pander_lme(rr_prof_task_gca_raw)

```

***

# Discussion

***

```{r plot-three-way-interaction, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, eval = FALSE}

# read in the raw datasets
rec_plot = dget('./data/plotting_df-pac.csv') %>%
  mutate_all(funs(as.numeric(.))) %>%
  mutate(lag = lag/20)

# split plotting dataframe by partner type
condition_dfs = split(rec_plot, 
                      rec_plot$partner_type)
friend_data = condition_dfs[[1]]
prof_data = condition_dfs[[2]]

# plot friend data
friend_plot = ggplot(friend_data,
                     aes(x=lag,
                         y=rr,
                         color=as.factor(task))) +
  scale_colour_viridis(name = "Task",
                       breaks=c(1,
                               2,
                               3,
                               4,
                               5),
                      labels=c('Convers.',
                               'Map - Participant',
                               'Map - Partner',
                               'Role',
                               'Tweety'),
                      discrete=TRUE) +
  theme(legend.position = "none") +
  stat_smooth() +
  ggtitle('Friend') +
  xlab(paste('Lag (in sec; ', downsampled_sampling_rate,' Hz)', sep="")) + ylab('Mean RR') +
  coord_cartesian(ylim = c(0, 
                           max(c(mean(friend_data$rr),mean(prof_data$rr))) + .1))

# plot professor data
prof_plot = ggplot(prof_data,
                     aes(x=lag,
                         y=rr,
                         color=as.factor(task))) +
  scale_colour_viridis(name = "Task",
                       breaks=c(1,
                               2,
                               3,
                               4,
                               5),
                      labels=c('Convers.',
                               'Map - Participant',
                               'Map - Partner',
                               'Role',
                               'Tweety'),
                      discrete=TRUE) +
  theme(legend.position = "none") +
  stat_smooth() +
  ggtitle('Professor') +
  xlab(paste('Lag (in sec; ', downsampled_sampling_rate,' Hz)', sep="")) + ylab('') +
  coord_cartesian(ylim = c(0, 
                           max(c(mean(friend_data$rr),mean(prof_data$rr))) + .1))

# create another noise plot just for the legend
friend_plot_legend = ggplot(friend_data,
                           aes(x=lag,
                               y=rr,
                               color=as.factor(task))) +
  scale_colour_viridis(name = "Task",
                       breaks=c(1,
                               2,
                               3,
                               4,
                               5),
                      labels=c('Convers.',
                               'Map - Participant',
                               'Map - Partner',
                               'Role',
                               'Tweety'),
                      discrete=TRUE) +
  ggtitle('Friend') +
  stat_smooth() +
  coord_cartesian(ylim = c(0, 
                           max(c(mean(friend_data$rr),mean(prof_data$rr))) + .1))

# create a master legend
master_legend = gtable_filter(ggplot_gtable(
  ggplot_build(friend_plot_legend + 
                 theme(legend.position="bottom"))),
  "guide-box")

# save to file for manuscript submission
ggsave('./figures/pac-interaction-RR_lag_partner_task.png',
       units = "in", width = 5.5, height = 5,
       grid.arrange(
         top=textGrob("Body movement synchrony\nby partner type and task",
                      gp=gpar(fontsize=14)),
         friend_plot,
         prof_plot,
         bottom=master_legend,
         ncol = 2
       ))

# save smaller version for knitr
ggsave('./figures/pac-interaction-RR_lag_partner_task-knitr.png',
       units = "in", width = 5.5, height = 5, dpi = 100,
       grid.arrange(
         top=textGrob("Body movement synchrony\nby partner type and task",
                      gp=gpar(fontsize=14)),
         friend_plot,
         prof_plot,
         bottom=master_legend,
         ncol = 2
       ))

# save tiff version
tiff('./figures/pac-interaction-RR_lag_partner_task.tiff',
     units = "in", width = 5.5, height = 5, res=300)
grid.arrange(
  top=textGrob("Body movement synchrony\nby partner type and task",
               gp=gpar(fontsize=14)),
  friend_plot,
  prof_plot,
  bottom=master_legend,
  ncol = 2
)
dev.off()


```

![**Figure**. Diagonal recurrence profiles (DRPs) for friends (left panel) and professor (right panel) by interaction type (purple = conversation; blue = map task; green = role-play; yellow = Tweety vignette retelling) within a 5-second window (sampled at 20Hz). In both panels, peaks on negative lags indiciate that the particiant was leading, and peaks on the positive lags indicate that the partner was leading.](./figures/pac-interaction-RR_lag_partner_task-knitr.png)